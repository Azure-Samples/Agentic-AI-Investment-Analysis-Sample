"""
Chat API endpoints for real-time communication with SSE support
"""
import asyncio
import json
import uuid
from datetime import datetime
from typing import List, Dict, Any, Optional, AsyncGenerator
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sse_starlette.sse import EventSourceResponse

router = APIRouter(prefix="/chat", tags=["chat"])

# In-memory storage (replace with database in production)
active_streams: Dict[str, asyncio.Queue] = {}
threads_storage: Dict[str, Dict[str, Any]] = {}


# Request/Response Models
class Message(BaseModel):
    role: str
    type: str
    content: Optional[str] = None
    timestamp: Optional[datetime] = None
    # Additional fields based on message type
    metadata: Optional[Dict[str, Any]] = None


class SendMessageRequest(BaseModel):
    message: str
    threadId: Optional[str] = None
    messageHistory: List[Dict[str, Any]] = Field(default_factory=list)
    userId: Optional[str] = None
    context: Optional[Dict[str, Any]] = Field(default_factory=dict)


class StreamInitResponse(BaseModel):
    streamId: str
    threadId: str
    status: str = "initiated"


class ChatResponse(BaseModel):
    threadId: str
    message: Optional[Dict[str, Any]] = None
    messages: Optional[List[Dict[str, Any]]] = None
    content: Optional[str] = None


class StreamChunk(BaseModel):
    type: str  # "message", "thinking", "error", "complete"
    content: Optional[str] = None
    message: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


# Helper Functions
async def process_message_with_streaming(
    request: SendMessageRequest,
    stream_id: str
) -> AsyncGenerator[str, None]:
    """
    Process message and yield streaming chunks
    This is where you'd integrate with your AI/agent workflow
    """
    try:
        # Access message history and user input
        message_count = len(request.messageHistory)
        user_message = request.message
        context = request.context or {}
        
        # TODO: Integrate with your actual AI/Agent workflow here
        # Example integration:
        # from app.workflow.investment_workflow import InvestmentWorkflow
        # workflow = InvestmentWorkflow()
        # result = await workflow.execute({
        #     "question": user_message,
        #     "message_history": request.messageHistory,
        #     "context": context
        # })
        
        # Example: Simulate agent processing with streaming
        
        # 1. Send thinking message (acknowledge conversation history)
        thinking_steps = [
            {
                "step": 1,
                "description": f"Analyzing your request with {message_count} previous messages in context...",
                "result": "Request parsed successfully"
            }
        ]
        
        # Add context awareness if there's history
        if message_count > 0:
            thinking_steps.append({
                "step": 2,
                "description": "Reviewing conversation history for context...",
                "result": f"Found {message_count} previous messages to consider"
            })
        
        thinking_message = {
            "role": "assistant",
            "type": "reasoning",
            "steps": thinking_steps
        }
        
        chunk = StreamChunk(
            type="thinking",
            message=thinking_message
        )
        yield f"{chunk.model_dump_json()}\n\n"
        await asyncio.sleep(1)
        
        # 2. Stream response content in chunks
        # Build context-aware response
        context_note = ""
        if message_count > 0:
            context_note = f"Continuing our conversation (message {message_count + 1}), "
        
        response_text = (
            f"{context_note}based on your query: '{user_message}', "
            "I'm analyzing the investment opportunity. "
            "This is a simulated streaming response that demonstrates how "
            "content can be sent in real-time as it's generated by the AI agents. "
        )
        
        # Add reference to message history if available
        if message_count > 2:
            response_text += (
                f"I've considered our previous {message_count} messages in this analysis. "
            )
        
        words = response_text.split()
        accumulated = ""
        
        for i, word in enumerate(words):
            accumulated += word + " "
            chunk = StreamChunk(
                type="message",
                content=word + " "
            )
            yield f"{chunk.model_dump_json()}\n\n"
            await asyncio.sleep(0.05)  # Simulate processing time
        
        # 3. Send additional message types (card, event, etc.)
        card_message = {
            "role": "assistant",
            "type": "card",
            "title": "Investment Summary",
            "content": "Key metrics and analysis results",
            "metrics": [
                {"label": "ROI Potential", "value": "28%", "trend": "up"},
                {"label": "Risk Level", "value": "Medium", "trend": "neutral"}
            ]
        }
        
        chunk = StreamChunk(
            type="thinking",
            message=card_message
        )
        yield f"{chunk.model_dump_json()}\n\n"
        await asyncio.sleep(1)
        
        # 4. Send completion
        final_message = {
            "role": "assistant",
            "type": "text",
            "content": accumulated.strip(),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        chunk = StreamChunk(
            type="complete",
            message=final_message
        )
        yield f"{chunk.model_dump_json()}\n\n"
        
    except Exception as e:
        error_chunk = StreamChunk(
            type="error",
            error=str(e)
        )
        yield f"{error_chunk.model_dump_json()}\n\n"


async def process_message_simple(
    request: SendMessageRequest
) -> ChatResponse:
    """
    Process message without streaming (fallback mode)
    """
    thread_id = request.threadId or str(uuid.uuid4())
    message_count = len(request.messageHistory)
    
    # TODO: Integrate with your AI/agent workflow here
    # Example:
    # from app.workflow.investment_workflow import InvestmentWorkflow
    # workflow = InvestmentWorkflow()
    # result = await workflow.execute({
    #     "question": request.message,
    #     "message_history": request.messageHistory,
    #     "context": request.context
    # })
    
    # For now, return a context-aware response
    context_note = f" (with {message_count} previous messages)" if message_count > 0 else ""
    
    response_message = {
        "role": "assistant",
        "type": "text",
        "content": f"Received your message{context_note}: {request.message}. This is a non-streaming response.",
        "timestamp": datetime.utcnow().isoformat()
    }
    
    return ChatResponse(
        threadId=thread_id,
        message=response_message
    )


# API Endpoints

@router.post("/stream", response_model=StreamInitResponse)
async def initiate_stream(request: SendMessageRequest):
    """
    Initiate a streaming chat session
    Returns a stream ID that can be used to connect to the SSE endpoint
    """
    stream_id = str(uuid.uuid4())
    thread_id = request.threadId or str(uuid.uuid4())
    
    # Store the request data for the stream endpoint to use
    active_streams[stream_id] = {
        "request": request,
        "thread_id": thread_id,
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Store thread data
    if thread_id not in threads_storage:
        threads_storage[thread_id] = {
            "id": thread_id,
            "messages": [],
            "created_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat()
        }
    
    # Initialize thread with message history if this is a new thread
    if len(threads_storage[thread_id]["messages"]) == 0 and request.messageHistory:
        # Store the message history in the thread
        threads_storage[thread_id]["messages"] = request.messageHistory
    
    # Add current user message to thread
    user_message = {
        "role": "user",
        "type": "text",
        "content": request.message,
        "timestamp": datetime.utcnow().isoformat()
    }
    threads_storage[thread_id]["messages"].append(user_message)
    threads_storage[thread_id]["updated_at"] = datetime.utcnow().isoformat()
    
    return StreamInitResponse(
        streamId=stream_id,
        threadId=thread_id,
        status="initiated"
    )


@router.get("/stream/{stream_id}")
async def stream_response(stream_id: str, request: Request):
    """
    SSE endpoint for streaming chat responses
    """
    if stream_id not in active_streams:
        raise HTTPException(status_code=404, detail="Stream not found")
    
    # Retrieve the stored request data
    stream_data = active_streams[stream_id]
    original_request = stream_data["request"]
    
    async def event_generator():
        try:
            async for chunk in process_message_with_streaming(original_request, stream_id):
                # Check if client disconnected
                if await request.is_disconnected():
                    break
                yield chunk
        except Exception as e:
            error_chunk = StreamChunk(type="error", error=str(e))
            yield f"{error_chunk.model_dump_json()}\n\n"
        finally:
            # Cleanup stream data after streaming completes
            if stream_id in active_streams:
                del active_streams[stream_id]
    
    return EventSourceResponse(event_generator())


@router.post("/message", response_model=ChatResponse)
async def send_message(request: SendMessageRequest):
    """
    Send a message without streaming (simple POST/response)
    """
    return await process_message_simple(request)


# Thread Management Endpoints

@router.post("/threads")
async def create_thread(title: Optional[str] = None, initialMessage: Optional[str] = None):
    """Create a new chat thread"""
    thread_id = str(uuid.uuid4())
    thread = {
        "id": thread_id,
        "title": title or "New Conversation",
        "preview": initialMessage or "Start a new conversation...",
        "timestamp": datetime.utcnow().isoformat(),
        "messageCount": 0,
        "messages": [],
        "tags": ["New"]
    }
    threads_storage[thread_id] = thread
    
    return {"thread": thread, "threadId": thread_id}


@router.get("/threads/{thread_id}")
async def get_thread(thread_id: str):
    """Get a specific thread by ID"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    return {"thread": threads_storage[thread_id]}


@router.get("/threads")
async def list_threads(page: int = 1, pageSize: int = 20):
    """List all threads"""
    threads_list = list(threads_storage.values())
    total = len(threads_list)
    
    start = (page - 1) * pageSize
    end = start + pageSize
    
    return {
        "threads": threads_list[start:end],
        "total": total,
        "page": page,
        "pageSize": pageSize
    }


@router.delete("/threads/{thread_id}")
async def delete_thread(thread_id: str):
    """Delete a thread"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    del threads_storage[thread_id]
    
    return {"success": True, "message": "Thread deleted successfully"}


@router.patch("/threads/{thread_id}")
async def update_thread(thread_id: str, updates: Dict[str, Any]):
    """Update thread metadata"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    threads_storage[thread_id].update(updates)
    threads_storage[thread_id]["updated_at"] = datetime.utcnow().isoformat()
    
    return {"thread": threads_storage[thread_id]}


@router.get("/threads/{thread_id}/messages")
async def get_messages(thread_id: str):
    """Get message history for a thread"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    return {"messages": threads_storage[thread_id].get("messages", [])}


@router.post("/feedback")
async def send_feedback(
    threadId: str,
    messageId: str,
    feedbackType: str,
    response: Dict[str, Any]
):
    """Handle human-in-the-loop feedback"""
    # TODO: Process feedback and continue workflow
    
    return {
        "success": True,
        "message": f"Feedback received for {feedbackType}"
    }


@router.post("/threads/{thread_id}/messages/{message_id}/regenerate")
async def regenerate_message(thread_id: str, message_id: str):
    """Regenerate the last assistant message"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    # TODO: Regenerate the message
    new_message = {
        "role": "assistant",
        "type": "text",
        "content": "This is a regenerated response.",
        "timestamp": datetime.utcnow().isoformat()
    }
    
    return {"message": new_message}
