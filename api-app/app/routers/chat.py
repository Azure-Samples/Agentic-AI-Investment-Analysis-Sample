"""
Chat API endpoints for real-time communication with SSE support
"""
import asyncio
import json
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, AsyncGenerator
from fastapi import APIRouter, Depends, BackgroundTasks, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from app.core.auth import get_current_active_user
from app.services import AnalysisService, WhatIfWorkflowExecutorService
from app.dependencies import (close_sse_event_queue_for_session, get_analysis_service, 
                              get_sse_event_queue_for_session, 
                              get_what_if_workflow_executor_service)
from app.models import User

router = APIRouter(prefix="/chat", tags=["chat"])

# In-memory storage (replace with database in production)
active_streams: Dict[str, asyncio.Queue] = {}
threads_storage: Dict[str, Dict[str, Any]] = {}

# region Request/Response Models

class ChatMessageRequest(BaseModel):
    message: str
    conversationId: Optional[str] = None
    messageHistory: List[Dict[str, Any]] = Field(default_factory=list)
    userId: Optional[str] = None
    context: Optional[Dict[str, Any]] = Field(default_factory=dict)


class StreamInitResponse(BaseModel):
    streamId: str
    conversationId: str
    status: str = "initiated"


class ChatResponse(BaseModel):
    conversationId: str
    message: Optional[Dict[str, Any]] = None
    messages: Optional[List[Dict[str, Any]]] = None
    content: Optional[str] = None


# class StreamChunk(BaseModel):
#     type: str  # "message", "thinking", "error", "complete"
#     content: Optional[str] = None
#     message: Optional[Dict[str, Any]] = None
#     error: Optional[str] = None
#     metadata: Optional[Dict[str, Any]] = None
#     conversationId: Optional[str] = None

# endregion

# # Helper Functions
# async def process_message_with_streaming(
#     request: ChatMessageRequest,
#     stream_id: str
# ) -> AsyncGenerator[str, None]:
#     """
#     Process message and yield streaming chunks
#     This is where you'd integrate with your AI/agent workflow
#     """
#     try:
#         # Access message history and user input
#         message_count = len(request.messageHistory)
#         user_message = request.message
#         context = request.context or {}
        
#         # TODO: Integrate with your actual AI/Agent workflow here
#         # Example integration:
#         # from app.workflow.investment_workflow import InvestmentWorkflow
#         # workflow = InvestmentWorkflow()
#         # result = await workflow.execute({
#         #     "question": user_message,
#         #     "message_history": request.messageHistory,
#         #     "context": context
#         # })
        
#         # Example: Simulate agent processing with streaming
        
#         # 1. Send thinking message (acknowledge conversation history)
#         thinking_steps = [
#             {
#                 "step": 1,
#                 "description": f"Analyzing your request with {message_count} previous messages in context...",
#                 "result": "Request parsed successfully"
#             }
#         ]
        
#         # Add context awareness if there's history
#         if message_count > 0:
#             thinking_steps.append({
#                 "step": 2,
#                 "description": "Reviewing conversation history for context...",
#                 "result": f"Found {message_count} previous messages to consider"
#             })
        
#         thinking_message = {
#             "role": "assistant",
#             "type": "reasoning",
#             "steps": thinking_steps
#         }
        
#         chunk = StreamChunk(
#             type="thinking",
#             message=thinking_message
#         )
#         yield f"{chunk.model_dump_json()}\n\n"
#         await asyncio.sleep(1)
        
#         # 2. Stream response content in chunks
#         # Build context-aware response
#         context_note = ""
#         if message_count > 0:
#             context_note = f"Continuing our conversation (message {message_count + 1}), "
        
#         response_text = (
#             f"{context_note}based on your query: '{user_message}', "
#             "I'm analyzing the investment opportunity. "
#             "This is a simulated streaming response that demonstrates how "
#             "content can be sent in real-time as it's generated by the AI agents. "
#         )
        
#         # Add reference to message history if available
#         if message_count > 2:
#             response_text += (
#                 f"I've considered our previous {message_count} messages in this analysis. "
#             )
        
#         words = response_text.split()
#         accumulated = ""
        
#         for i, word in enumerate(words):
#             accumulated += word + " "
#             chunk = StreamChunk(
#                 type="message",
#                 content=word + " "
#             )
#             yield f"{chunk.model_dump_json()}\n\n"
#             await asyncio.sleep(0.05)  # Simulate processing time
        
#         # 3. Send additional message types (card, event, etc.)
#         card_message = {
#             "role": "assistant",
#             "type": "card",
#             "title": "Investment Summary",
#             "content": "Key metrics and analysis results",
#             "metrics": [
#                 {"label": "ROI Potential", "value": "28%", "trend": "up"},
#                 {"label": "Risk Level", "value": "Medium", "trend": "neutral"}
#             ]
#         }
        
#         chunk = StreamChunk(
#             type="thinking",
#             message=card_message
#         )
#         yield f"{chunk.model_dump_json()}\n\n"
#         await asyncio.sleep(1)
        
#         # 4. Send completion
#         final_message = {
#             "role": "assistant",
#             "type": "text",
#             "content": accumulated.strip(),
#             "timestamp": datetime.utcnow().isoformat()
#         }
        
#         chunk = StreamChunk(
#             type="complete",
#             message=final_message
#         )
#         yield f"{chunk.model_dump_json()}\n\n"
        
#     except Exception as e:
#         error_chunk = StreamChunk(
#             type="error",
#             error=str(e)
#         )
#         yield f"{error_chunk.model_dump_json()}\n\n"



# API Endpoints

# region Chat Endpoints

@router.post("/stream", response_model=StreamInitResponse)
async def initiate_stream(
    request: ChatMessageRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_active_user),
    execution_service: WhatIfWorkflowExecutorService = Depends(get_what_if_workflow_executor_service),):
    """
    Initiate a streaming chat session
    Returns a stream ID that can be used to connect to the SSE endpoint
    """
    stream_id = str(uuid.uuid4()) # Unique stream ID, used by the client to connect, could also use session ID.
    conversation_id = request.conversationId or str(uuid.uuid4())
    
    analysis_id = request.context.get("analysisId")
    opportunity_id = request.context.get("opportunityId")
    if not analysis_id or not opportunity_id:
        raise HTTPException(status_code=400, detail="analysisId and opportunityId must be provided in request's context property.")
    
    # Store the request data for the stream endpoint to use
    active_streams[stream_id] = {
        "request": request,
        "conversation_id": conversation_id,
        "created_at": datetime.now(timezone.utc).isoformat()
    }
    
    # Get the event queue for this client/session
    event_queue = await get_sse_event_queue_for_session(stream_id)
    
    # Execute workflow in background
    workflow_executor_function = execution_service.execute_workflow
    background_tasks.add_task(
            workflow_executor_function,
            input_message=request.message,
            conversation_id=conversation_id,
            sse_event_queue=event_queue,
            analysis_id=analysis_id,
            opportunity_id=opportunity_id,
            owner_id=current_user.email
        )

    return StreamInitResponse(
        streamId=stream_id,
        conversationId=conversation_id,
        status="initiated"
    )


@router.get("/stream/{stream_id}")
async def stream_response(stream_id: str):
    """
    SSE endpoint for streaming chat responses
    """
    if stream_id not in active_streams:
        raise HTTPException(status_code=404, detail="Stream not found")
    
    # # Retrieve the stored request data
    # stream_data = active_streams[stream_id]
    
    # Get the event queue for this client/session
    event_queue = await get_sse_event_queue_for_session(stream_id)
        
    async def clean_up():
        await close_sse_event_queue_for_session(stream_id)
    
    async def event_generator() -> AsyncGenerator[str, None]:
                
        try:
            # Send all existing events
            all_events = await event_queue.get_events()
            for event in all_events:
                yield event.to_sse_format()
                
            # Register for live updates
            listener_queue = await event_queue.register_listener()
                
            try:
                # Stream live events
                while True:
                    try:
                        # Wait for new events with timeout to allow for keep-alive
                        event = await asyncio.wait_for(listener_queue.get(), timeout=30.0)
                        yield event.to_sse_format()
                            
                    except asyncio.TimeoutError:
                        # Send keep-alive comment to prevent connection timeout
                        yield ": keep-alive\n\n"
                            
            except asyncio.CancelledError:
                raise
            finally:
                # Cleanup listener
                await event_queue.unregister_listener(listener_queue)
                    
        except Exception as e:
            # Send error event
            error_data = f'data: {{"type": "error", "message": "Stream error: {str(e)}", "data":  {{"error": "{str(e)}", "error_type": "{type(e).__name__}"}} , "timestamp": "{datetime.now(timezone.utc).isoformat()}"}}\n\n'
            yield error_data
                
        finally:
            await clean_up()
    
    return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # Disable buffering in nginx
                "Access-Control-Allow-Origin": "*",  # Allow CORS for SSE
                "Access-Control-Allow-Credentials": "true"
            }
        )


@router.post("/message", response_model=ChatResponse)
async def send_message(request: ChatMessageRequest):
    """
    Send a message without streaming (simple POST/response)
    """
    pass

# end region

# Thread Management Endpoints

@router.get("/threads/{thread_id}")
async def get_thread(thread_id: str):
    """Get a specific thread by ID"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    return {"thread": threads_storage[thread_id]}


@router.get("/coversations")
async def list_conversations(page: int = 1, pageSize: int = 20):
    """List all conversations with pagination"""
    threads_list = list(threads_storage.values())
    total = len(threads_list)
    
    start = (page - 1) * pageSize
    end = start + pageSize
    
    return {
        "threads": threads_list[start:end],
        "total": total,
        "page": page,
        "pageSize": pageSize
    }


@router.delete("/threads/{thread_id}")
async def delete_thread(thread_id: str):
    """Delete a thread"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    del threads_storage[thread_id]
    
    return {"success": True, "message": "Thread deleted successfully"}


@router.patch("/threads/{thread_id}")
async def update_thread(thread_id: str, updates: Dict[str, Any]):
    """Update thread metadata"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    threads_storage[thread_id].update(updates)
    threads_storage[thread_id]["updated_at"] = datetime.utcnow().isoformat()
    
    return {"thread": threads_storage[thread_id]}


@router.get("/threads/{thread_id}/messages")
async def get_messages(thread_id: str):
    """Get message history for a thread"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    return {"messages": threads_storage[thread_id].get("messages", [])}


@router.post("/feedback")
async def send_feedback(
    threadId: str,
    messageId: str,
    feedbackType: str,
    response: Dict[str, Any]
):
    """Handle human-in-the-loop feedback"""
    # TODO: Process feedback and continue workflow
    
    return {
        "success": True,
        "message": f"Feedback received for {feedbackType}"
    }


@router.post("/threads/{thread_id}/messages/{message_id}/regenerate")
async def regenerate_message(thread_id: str, message_id: str):
    """Regenerate the last assistant message"""
    if thread_id not in threads_storage:
        raise HTTPException(status_code=404, detail="Thread not found")
    
    # TODO: Regenerate the message
    new_message = {
        "role": "assistant",
        "type": "text",
        "content": "This is a regenerated response.",
        "timestamp": datetime.utcnow().isoformat()
    }
    
    return {"message": new_message}
